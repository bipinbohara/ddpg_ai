{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Deterministic Policy Gradient (DDPG), Reinforcement Learning.\n",
    "P2P network, net bit rate, energy harvesting example for training.\n",
    "Thanks to : https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG\n",
    "Using:\n",
    "tensorflow 1.0\n",
    "\"\"\"\n",
    "import math\n",
    "#import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import EH_P2P\n",
    "import DDPG_CLASS\n",
    "#from .DDPG_CLASS import * \n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 2000\n",
    "MAX_EP_STEPS = 120\n",
    "LR_A = 0.0002   # learning rate for actor\n",
    "LR_C = 0.0002  # learning rate for critic\n",
    "GAMMA = 0.999    # reward discount\n",
    "REPLACEMENT = [\n",
    "    dict(name='soft', tau=0.01),\n",
    "    dict(name='hard', rep_iter_a=600, rep_iter_c=500)\n",
    "][0]            # you can try different target replacement strategies\n",
    "MEMORY_CAPACITY = 40000\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "    \n",
    "env=EH_P2P.EH_P2P()\n",
    "env.Chanpower()\n",
    "env.Solarread()\n",
    "    \n",
    "state_dim = 3 #channel,batteryï¼Œsolar\n",
    "action_dim = 1 #Transmission power\n",
    "action_bound = 1 #[0,1]\n",
    "\n",
    "tip=1\n",
    "tip2=1\n",
    "snr=-10\n",
    "\n",
    "for temp in range(1):\n",
    "    for snr in range(-10,10,2):\n",
    "    #for modulation in range(1):\n",
    "        var = 10\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        with tf.name_scope('S'):\n",
    "            S = tf.placeholder(tf.float32, shape=[None, state_dim], name='s')\n",
    "        with tf.name_scope('R'):\n",
    "            R = tf.placeholder(tf.float32, [None, 1], name='r')\n",
    "        with tf.name_scope('S_'):\n",
    "            S_ = tf.placeholder(tf.float32, shape=[None, state_dim], name='s_')\n",
    "        DDPG_CLASS.S=S\n",
    "        DDPG_CLASS.R=R\n",
    "        DDPG_CLASS.S_=S_\n",
    "        actor = PPOActor.Actor(sess, action_dim, action_bound, LR_A, REPLACEMENT)\n",
    "        critic = DDPG_CLASS.Critic(sess, state_dim, action_dim, LR_C, GAMMA, REPLACEMENT, actor.a, actor.a_)\n",
    "        actor.add_grad_to_graph(critic.a_grads)\n",
    "        M = DDPG_CLASS.Memory(MEMORY_CAPACITY, dims=2 * state_dim + action_dim + 1)\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver=tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "        if OUTPUT_GRAPH:\n",
    "            tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "        print(\"modulation=\",modulation,\"snr=\",snr)\n",
    "\n",
    "        for i in range(MAX_EPISODES):\n",
    "    \n",
    "            s = env.reset_P2P(snr)\n",
    "\n",
    "            ep_reward = 0\n",
    "            for j in range(MAX_EP_STEPS):\n",
    "                a = actor.choose_action(s)\n",
    "                a = np.random.normal(a, var)\n",
    "                a=np.clip(a,0,1)\n",
    "                s_, r, info = env.step_P2P([a,modulation])#input modulation 0:qpsk,1:8psk,2:16qam\n",
    "\n",
    "                M.store_transition(s, a, r , s_)\n",
    "\n",
    "                if M.pointer > MEMORY_CAPACITY:\n",
    "                    #tip and tip2 are only for printing`#\n",
    "                    if tip == 1:\n",
    "                        print(\"memory full\",j,i)\n",
    "                        tip=0\n",
    "                    var *= 0.9995  # decay the action randomness\n",
    "                    if tip2 == 1 and var<0.00000001:\n",
    "                        print(\"var zero\",j,i)\n",
    "                        tip2=0\n",
    "                        \n",
    "                    b_M = M.sample(BATCH_SIZE)\n",
    "                    b_s = b_M[:, :state_dim]\n",
    "                    b_a = b_M[:, state_dim: state_dim + action_dim]\n",
    "                    b_r = b_M[:, -state_dim - 1: -state_dim]\n",
    "                    b_s_ = b_M[:, -state_dim:]\n",
    "\n",
    "                    critic.learn(b_s, b_a, b_r, b_s_)\n",
    "                    actor.learn(b_s)\n",
    "\n",
    "                s = s_\n",
    "                \n",
    "                ep_reward += r\n",
    "\n",
    "\n",
    "            if i % 30 == 0 :\n",
    "                print(\"net bit rate=\",r,\"action\",a, \"solar,channel,battery\",s,\"epoch\",i)\n",
    "                print(\"ave_reward\",ep_reward/(j+1))\n",
    "                \n",
    "                \n",
    "                \n",
    "        save_path = saver.save(sess, \"folder_for_nn_noise\"+\"/EH_save_net_snr=\"+str(snr)+str(modulation)+\"epoch=\"+str(i)+\"_P2P.ckpt\")\n",
    "        print(\"Save to path: \", save_path)\n",
    "\n",
    "print(\"----------------------------END--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
