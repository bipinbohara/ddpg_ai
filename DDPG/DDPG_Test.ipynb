{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "INFO:tensorflow:Restoring parameters from folder_for_nn_noise/EH_save_net_snr=-100epoch=1999_P2P.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732833756.139955  243944 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/client/session.py:1483: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `fetch` = Actor/eval_net/a/a/kernel:0 cannot be interpreted as a Tensor. (\"The name 'Actor/eval_net/a/a/kernel:0' refers to a Tensor which does not exist. The operation, 'Actor/eval_net/a/a/kernel', does not exist in the graph.\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:2944\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2944\u001b[0m   op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_operation_by_name(op_name)\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/client/session.py:313\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unique_fetches\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39mas_graph_element(\n\u001b[1;32m    314\u001b[0m       fetch, allow_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_operation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:2904\u001b[0m, in \u001b[0;36mGraph.as_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 2904\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:2946\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m-> 2946\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   2947\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m refers to a Tensor which does not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2948\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexist. The operation, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, does not exist in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2949\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(name), \u001b[38;5;28mrepr\u001b[39m(op_name))\n\u001b[1;32m   2950\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'Actor/eval_net/a/a/kernel:0' refers to a Tensor which does not exist. The operation, 'Actor/eval_net/a/a/kernel', does not exist in the graph.\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m a3 \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActor/eval_net/l3/kernel:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m b3 \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActor/eval_net/l3/bias:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m aa \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActor/eval_net/a/a/kernel:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m ba \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActor/eval_net/a/a/bias:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_EPISODES):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/client/session.py:977\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    974\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;28;01mNone\u001b[39;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    978\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    979\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    980\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/client/session.py:1205\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1202\u001b[0m       feed_map[compat\u001b[38;5;241m.\u001b[39mas_bytes(subfeed_t\u001b[38;5;241m.\u001b[39mname)] \u001b[38;5;241m=\u001b[39m (subfeed_t, subfeed_val)\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;66;03m# Create a fetch handler to take care of the structure of fetches.\u001b[39;00m\n\u001b[0;32m-> 1205\u001b[0m fetch_handler \u001b[38;5;241m=\u001b[39m _FetchHandler(\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph, fetches, feed_dict_tensor, feed_handles\u001b[38;5;241m=\u001b[39mfeed_handles)\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# Run request and get response.\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;66;03m# We need to keep the returned movers alive for the following _do_run().\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# These movers are no longer needed when _do_run() completes, and\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# are deleted when `movers` goes out of scope when this _run() ends.\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# of a handle from a different device as an error.\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_with_movers(feed_dict_tensor, feed_map)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/client/session.py:494\u001b[0m, in \u001b[0;36m_FetchHandler.__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a fetch handler.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m    direct feeds.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 494\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_mapper \u001b[38;5;241m=\u001b[39m _FetchMapper\u001b[38;5;241m.\u001b[39mfor_fetch(fetches)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_targets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/client/session.py:285\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fetch, tensor_type):\n\u001b[1;32m    284\u001b[0m       fetches, contraction_fn \u001b[38;5;241m=\u001b[39m fetch_fn(fetch)\n\u001b[0;32m--> 285\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _ElementFetchMapper(fetches, contraction_fn)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Did not find anything.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has invalid type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    288\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(fetch)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/client/session.py:323\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be interpreted as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    321\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma Tensor. (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    322\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be interpreted as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    324\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma Tensor. (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contraction_fn \u001b[38;5;241m=\u001b[39m contraction_fn\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `fetch` = Actor/eval_net/a/a/kernel:0 cannot be interpreted as a Tensor. (\"The name 'Actor/eval_net/a/a/kernel:0' refers to a Tensor which does not exist. The operation, 'Actor/eval_net/a/a/kernel', does not exist in the graph.\")"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Deterministic Policy Gradient (DDPG), Reinforcement Learning.\n",
    "P2P, net bit rate, energy harvesting example for validation.\n",
    "Using:\n",
    "tensorflow 1.0\n",
    "\"\"\"\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import EH_P2P\n",
    "import math\n",
    "\n",
    "MAX_EPISODES = 1\n",
    "MAX_EP_STEPS = 100000\n",
    "\n",
    "RENDER = False\n",
    "OUTPUT_GRAPH = True\n",
    "\n",
    "env=EH_P2P.EH_P2P()\n",
    "env.Chanpower()\n",
    "env.Solarread()\n",
    "B=np.zeros((100,4))    \n",
    "\n",
    "def dense(x,a,b):\n",
    "    results=(x.dot(a)+b)\n",
    "    return results\n",
    "\n",
    "def choose_action(s):\n",
    "    a=dense(s,a1,b1)\n",
    "    a=dense(a,a3,b3)\n",
    "    a=dense(a,aa,ba)\n",
    "    a=1/(1+np.exp(-a))\n",
    "    a = np.clip(a, 0, 1)\n",
    "    return a\n",
    "\n",
    "for snr in range (-10,-8,2):\n",
    "\n",
    "    for epoch in range (0,1750,30):\n",
    "\n",
    "        modulation=0\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        saver = tf.train.import_meta_graph(\"folder_for_nn_noise\"+\"/EH_save_net_snr=\"+str(snr)+str(modulation)+\"epoch=\"+str(epoch)+\"_P2P.ckpt.meta\")\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess,\"folder_for_nn_noise\"+\"/EH_save_net_snr=\"+str(snr)+str(modulation)+\"epoch=\"+str(epoch)+\"_P2P.ckpt\")\n",
    "\n",
    "            a1 = sess.run('Actor/eval_net/l1/kernel:0')\n",
    "            b1 = sess.run('Actor/eval_net/l1/bias:0')\n",
    "            a3 = sess.run('Actor/eval_net/l3/kernel:0')\n",
    "            b3 = sess.run('Actor/eval_net/l3/bias:0')\n",
    "            aa = sess.run('Actor/eval_net/a/a/kernel:0')\n",
    "            ba = sess.run('Actor/eval_net/a/a/bias:0')\n",
    "    \n",
    "\n",
    "    \n",
    "            for i in range(MAX_EPISODES):\n",
    "\n",
    "                s = env.reset_P2P(snr=snr)\n",
    "                s=np.reshape(s,(1,-1))\n",
    "                ep_reward = 0\n",
    "                for j in range(MAX_EP_STEPS):\n",
    "                    s=np.array(s,dtype=float)\n",
    "                    a=choose_action(s)\n",
    "                    s_, r, info = env.step_P2P([a,modulation])\n",
    "                    s = s_\n",
    "                    ep_reward += r\n",
    "                    if (j+1)%10000==0:\n",
    "                        print(\"net bit rate=\",ep_reward/j,\"snr=\",snr,\"modulation=\",modulation, \"loop =\",i,\"action=\",a,\"noise=\",env.noise)\n",
    "          \n",
    "                index=(snr+10)/2\n",
    "                B[int(index),int(modulation)]=ep_reward/j\n",
    "                print(B[int(index),int(modulation)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from folder_for_nn_noise/EH_save_net_snr=-100epoch=1999_P2P.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yd/0j058frj781_p8p0d_z_jkxh0000gp/T/ipykernel_48113/3540547475.py:55: RuntimeWarning: overflow encountered in exp\n",
      "  a=1/(1+np.exp(-a))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net bit rate= 19011.679435104754 snr= -10 modulation= 0 loop = 0 action= [1.] noise= 10.0\n",
      "net bit rate= 21338.79836213287 snr= -10 modulation= 0 loop = 0 action= [1.] noise= 10.0\n",
      "net bit rate= 21936.615485440852 snr= -10 modulation= 0 loop = 0 action= [1.] noise= 10.0\n",
      "net bit rate= 21226.6695211434 snr= -10 modulation= 0 loop = 0 action= [1.] noise= 10.0\n",
      "net bit rate= 20805.765094082057 snr= -10 modulation= 0 loop = 0 action= [1.] noise= 10.0\n",
      "net bit rate= 20453.002982897902 snr= -10 modulation= 0 loop = 0 action= [1.] noise= 10.0\n",
      "net bit rate= 21116.767773178814 snr= -10 modulation= 0 loop = 0 action= [1.] noise= 10.0\n",
      "net bit rate= 21136.91274490488 snr= -10 modulation= 0 loop = 0 action= [0.] noise= 10.0\n",
      "net bit rate= 21254.6077308788 snr= -10 modulation= 0 loop = 0 action= [1.] noise= 10.0\n",
      "net bit rate= 21038.743658442476 snr= -10 modulation= 0 loop = 0 action= [1.] noise= 10.0\n",
      "21038.743658442476\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Deterministic Policy Gradient (DDPG), Reinforcement Learning.\n",
    "P2P, net bit rate, energy harvesting example for validation.\n",
    "Using:\n",
    "tensorflow 1.0\n",
    "\"\"\"\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import EH_P2P\n",
    "import math\n",
    "\n",
    "MAX_EPISODES = 1\n",
    "MAX_EP_STEPS = 100000\n",
    "\n",
    "RENDER = False\n",
    "OUTPUT_GRAPH = True\n",
    "\n",
    "env=EH_P2P.EH_P2P()\n",
    "env.Chanpower()\n",
    "env.Solarread()\n",
    "B=np.zeros((100,4))    \n",
    "\n",
    "def dense(x,a,b):\n",
    "    #new\n",
    "    # Ensure all inputs are NumPy arrays\n",
    "    x = np.array(x, dtype=float)\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "\n",
    "    # Check shapes for debugging\n",
    "    # print(f\"x shape: {x.shape}, a shape: {a.shape}, b shape: {b.shape}\")\n",
    "\n",
    "    results=(x.dot(a)+b)\n",
    "    return results\n",
    "\n",
    "def choose_action(s):\n",
    "    # Debug input shape\n",
    "    # print(f\"choose_action() - input s shape: {s.shape}\")\n",
    "\n",
    "    # # Fix input shape mismatch if necessary\n",
    "    # if s.shape[1] != a1.shape[0]:\n",
    "    #     print(f\"Padding input s from shape {s.shape} to match a1 shape {a1.shape}\")\n",
    "    #     s = np.pad(s, ((0, 0), (0, a1.shape[0] - s.shape[1])), 'constant')\n",
    "    \n",
    "   \n",
    "    a=dense(s,a1,b1)\n",
    "    a=dense(a,a3,b3)\n",
    "    a=dense(a,aa,ba)\n",
    "\n",
    "    a=1/(1+np.exp(-a))\n",
    "    a = np.clip(a, 0, 1)\n",
    "    return a\n",
    "\n",
    "\n",
    "snr=-10\n",
    "epoch=1999\n",
    "modulation=0\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"folder_for_nn_noise\"+\"/EH_save_net_snr=\"+str(snr)+str(modulation)+\"epoch=\"+str(epoch)+\"_P2P.ckpt.meta\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\"folder_for_nn_noise\"+\"/EH_save_net_snr=\"+str(snr)+str(modulation)+\"epoch=\"+str(epoch)+\"_P2P.ckpt\")\n",
    "\n",
    "    # for op in tf.get_default_graph().get_operations():\n",
    "    #     print(op.name, [tensor.shape for tensor in op.outputs])\n",
    "    \n",
    "\n",
    "    # Retrieve tensors and reshape to match expected dimensions\n",
    "    a1 = sess.run('Actor/eval_net/l1/kernel:0')\n",
    "    b1 = sess.run('Actor/eval_net/l1/bias:0')\n",
    "    a3 = sess.run('Actor/eval_net/l3/kernel:0')\n",
    "    b3 = sess.run('Actor/eval_net/l3/bias:0')\n",
    "    aa = sess.run('Actor/eval_net/a/kernel:0')\n",
    "    ba = sess.run('Actor/eval_net/a/bias:0')\n",
    "\n",
    "    # print(f\"Shape a1: {a1.shape}, Shape b1: {b1.shape}\")\n",
    "    # print(f\"Shape a3: {a3.shape}, Shape b3: {b3.shape}\")\n",
    "    # print(f\"Shape aa: {aa.shape}, Shape ba: {ba.shape}\")\n",
    "\n",
    "    # Dynamically adjust dimensions\n",
    "    input_dim = 3\n",
    "    output_dim = 1\n",
    "\n",
    "    # Compute hidden dimensions dynamically\n",
    "    hidden_dim1 = a1.size // input_dim\n",
    "    if a1.size % input_dim == 0:\n",
    "        a1 = a1.reshape(input_dim, hidden_dim1)\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot reshape a1 with size {a1.size} into ({input_dim}, hidden_dim1)\")\n",
    "\n",
    "    hidden_dim2 = a3.size // hidden_dim1\n",
    "    if a3.size % hidden_dim1 == 0:\n",
    "        a3 = a3.reshape(hidden_dim1, hidden_dim2)\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot reshape a3 with size {a3.size} into ({hidden_dim1}, hidden_dim2)\")\n",
    "\n",
    "    # Reshape biases dynamically\n",
    "    if b1.size >= hidden_dim1:\n",
    "        b1 = b1[:hidden_dim1]  # Slice to match hidden_dim1\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot reshape b1 with size {b1.size} into ({hidden_dim1},)\")\n",
    "\n",
    "    if b3.size >= hidden_dim2:\n",
    "        b3 = b3[:hidden_dim2]  # Slice to match hidden_dim2\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot reshape b3 with size {b3.size} into ({hidden_dim2},)\")\n",
    "\n",
    "    if ba.size >= output_dim:\n",
    "        ba = ba[:output_dim]  # Slice to match output_dim\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot reshape ba with size {ba.size} into ({output_dim},)\")\n",
    "\n",
    "    # Reshape output layer weights\n",
    "    if aa.size >= hidden_dim2 * output_dim:\n",
    "        aa = aa[:hidden_dim2 * output_dim].reshape(hidden_dim2, output_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot reshape aa with size {aa.size} into ({hidden_dim2}, {output_dim})\")\n",
    "\n",
    "    # a1 = a1.reshape(input_dim, hidden_dim1)\n",
    "    # b1 = b1.reshape(hidden_dim1)\n",
    "    # a3 = a3.reshape(hidden_dim1, hidden_dim2)\n",
    "    # b3 = b3.reshape(hidden_dim2)\n",
    "    # aa = aa.reshape(hidden_dim2, output_dim)\n",
    "    # ba = ba.reshape(output_dim)\n",
    "    \n",
    "    # # Reshape\n",
    "    # print(f\"Reshaped a1: {a1.shape}, Reshaped b1: {b1.shape}\")\n",
    "    # print(f\"Reshaped a3: {a3.shape}, Reshaped b3: {b3.shape}\")\n",
    "    # print(f\"Reshaped aa: {aa.shape}, Reshaped ba: {ba.shape}\")\n",
    "\n",
    "    for i in range(MAX_EPISODES):\n",
    "\n",
    "        s = env.reset_P2P(snr=snr)\n",
    "        s=np.reshape(s,(1,-1))\n",
    "        ep_reward = 0\n",
    "        for j in range(MAX_EP_STEPS):\n",
    "            s=np.array(s,dtype=float)\n",
    "            a=choose_action(s)\n",
    "            s_, r, info = env.step_P2P([a,modulation])\n",
    "            s = s_\n",
    "            ep_reward += r\n",
    "            if (j+1)%10000==0:\n",
    "                print(\"net bit rate=\",ep_reward/j,\"snr=\",snr,\"modulation=\",modulation, \"loop =\",i,\"action=\",a,\"noise=\",env.noise)\n",
    "    \n",
    "        index=(snr+10)/2\n",
    "        B[int(index),int(modulation)]=ep_reward/j\n",
    "        print(B[int(index),int(modulation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"DDPG_P2P_noise2\"+str(snr)+\".csv\", B, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras as k\n",
    "k.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
