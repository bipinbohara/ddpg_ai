{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:43: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n",
      "<>:43: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'set_random_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mDDPG_CLASS\u001b[39;00m\n\u001b[1;32m     16\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m tf\u001b[38;5;241m.\u001b[39mset_random_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#####################  hyper parameters  ####################\u001b[39;00m\n\u001b[1;32m     23\u001b[0m MAX_EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'set_random_seed'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Deterministic Policy Gradient (DDPG), Reinforcement Learning.\n",
    "P2P network, net bit rate, energy harvesting example for training.\n",
    "Thanks to : https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/9_Deep_Deterministic_Policy_Gradient_DDPG\n",
    "Using:\n",
    "tensorflow 1.0\n",
    "\"\"\"\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import EH_P2P\n",
    "import DDPG_CLASS\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 2000\n",
    "MAX_EP_STEPS = 120\n",
    "LR_A = 0.0002   # learning rate for actor\n",
    "LR_C = 0.0002  # learning rate for critic\n",
    "GAMMA = 0.999    # reward discount\n",
    "REPLACEMENT = [\n",
    "    dict(name='soft', tau=0.01),\n",
    "    dict(name='hard', rep_iter_a=600, rep_iter_c=500)\n",
    "][0]            # you can try different target replacement strategies\n",
    "MEMORY_CAPACITY = 40000\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "    \n",
    "env=EH_P2P.EH_P2P()\n",
    "env.Chanpower()\n",
    "env.Solarread()\n",
    "    \n",
    "state_dim = 3 #channel,batteryï¼Œsolar\n",
    "action_dim = 1 #Transmission power\n",
    "action_bound = 1 [0,1]\n",
    "\n",
    "tip=1\n",
    "tip2=1\n",
    "snr=-10\n",
    "\n",
    "for temp in range(1):\n",
    "   #for snr in range(-10,10,2):\n",
    "    for modulation in range(1):\n",
    "        var = 10\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        with tf.name_scope('S'):\n",
    "            S = tf.placeholder(tf.float32, shape=[None, state_dim], name='s')\n",
    "        with tf.name_scope('R'):\n",
    "            R = tf.placeholder(tf.float32, [None, 1], name='r')\n",
    "        with tf.name_scope('S_'):\n",
    "            S_ = tf.placeholder(tf.float32, shape=[None, state_dim], name='s_')\n",
    "        DDPG_CLASS.S=S\n",
    "        DDPG_CLASS.R=R\n",
    "        DDPG_CLASS.S_=S_\n",
    "        actor = DDPG_CLASS.Actor(sess, action_dim, action_bound, LR_A, REPLACEMENT)\n",
    "        critic = DDPG_CLASS.Critic(sess, state_dim, action_dim, LR_C, GAMMA, REPLACEMENT, actor.a, actor.a_)\n",
    "        actor.add_grad_to_graph(critic.a_grads)\n",
    "        M = DDPG_CLASS.Memory(MEMORY_CAPACITY, dims=2 * state_dim + action_dim + 1)\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver=tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "        if OUTPUT_GRAPH:\n",
    "            tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "        print(\"modulation=\",modulation,\"snr=\",snr)\n",
    "\n",
    "        for i in range(MAX_EPISODES):\n",
    "    \n",
    "            s = env.reset_P2P(snr)\n",
    "\n",
    "            ep_reward = 0\n",
    "            for j in range(MAX_EP_STEPS):\n",
    "                a = actor.choose_action(s)\n",
    "                a = np.random.normal(a, var)\n",
    "                a=np.clip(a,0,1)\n",
    "                s_, r, info = env.step_P2P([a,modulation])#input modulation 0:qpsk,1:8psk,2:16qam\n",
    "\n",
    "                M.store_transition(s, a, r , s_)\n",
    "\n",
    "                if M.pointer > MEMORY_CAPACITY:\n",
    "                    #tip and tip2 are only for printing`#\n",
    "                    if tip == 1:\n",
    "                        print(\"memory full\",j,i)\n",
    "                        tip=0\n",
    "                    var *= 0.9995  # decay the action randomness\n",
    "                    if tip2 == 1 and var<0.00000001:\n",
    "                        print(\"var zero\",j,i)\n",
    "                        tip2=0\n",
    "                        \n",
    "                    b_M = M.sample(BATCH_SIZE)\n",
    "                    b_s = b_M[:, :state_dim]\n",
    "                    b_a = b_M[:, state_dim: state_dim + action_dim]\n",
    "                    b_r = b_M[:, -state_dim - 1: -state_dim]\n",
    "                    b_s_ = b_M[:, -state_dim:]\n",
    "\n",
    "                    critic.learn(b_s, b_a, b_r, b_s_)\n",
    "                    actor.learn(b_s)\n",
    "\n",
    "                s = s_\n",
    "                \n",
    "                ep_reward += r\n",
    "\n",
    "\n",
    "            if i % 30 == 0 :\n",
    "                print(\"net bit rate=\",r,\"action\",a, \"solar,channel,battery\",s,\"epoch\",i)\n",
    "                print(\"ave_reward\",ep_reward/(j+1))\n",
    "                \n",
    "                \n",
    "                \n",
    "        save_path = saver.save(sess, \"folder_for_nn_noise\"+\"/EH_save_net_snr=\"+str(snr)+str(modulation)+\"epoch=\"+str(i)+\"_P2P.ckpt\")\n",
    "        print(\"Save to path: \", save_path)\n",
    "\n",
    "print(\"----------------------------END--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Actor.__init__() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m DDPG_CLASS\u001b[38;5;241m.\u001b[39mS_ \u001b[38;5;241m=\u001b[39m S_\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Actor and Critic initialization\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m actor \u001b[38;5;241m=\u001b[39m DDPG_CLASS\u001b[38;5;241m.\u001b[39mActor(state_dim, action_dim, action_bound, LR_A, REPLACEMENT)\n\u001b[1;32m     55\u001b[0m critic \u001b[38;5;241m=\u001b[39m DDPG_CLASS\u001b[38;5;241m.\u001b[39mCritic(state_dim, action_dim, LR_C, GAMMA, REPLACEMENT, actor\u001b[38;5;241m.\u001b[39ma, actor\u001b[38;5;241m.\u001b[39ma_)\n\u001b[1;32m     56\u001b[0m actor\u001b[38;5;241m.\u001b[39madd_grad_to_graph(critic\u001b[38;5;241m.\u001b[39ma_grads)\n",
      "\u001b[0;31mTypeError\u001b[0m: Actor.__init__() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import EH_P2P\n",
    "import DDPG_CLASS\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 2000\n",
    "MAX_EP_STEPS = 120\n",
    "LR_A = 0.0002   # learning rate for actor\n",
    "LR_C = 0.0002  # learning rate for critic\n",
    "GAMMA = 0.999    # reward discount\n",
    "REPLACEMENT = [\n",
    "    dict(name='soft', tau=0.01),\n",
    "    dict(name='hard', rep_iter_a=600, rep_iter_c=500)\n",
    "][0]            # you can try different target replacement strategies\n",
    "MEMORY_CAPACITY = 40000\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "    \n",
    "env = EH_P2P.EH_P2P()\n",
    "env.Chanpower()\n",
    "env.Solarread()\n",
    "    \n",
    "state_dim = 3 #channel, battery, solar\n",
    "action_dim = 1 #Transmission power\n",
    "action_bound = 1 #[0,1]\n",
    "\n",
    "tip = 1\n",
    "tip2 = 1\n",
    "snr = -10\n",
    "\n",
    "for temp in range(1):\n",
    "    for modulation in range(1):\n",
    "        var = 10\n",
    "\n",
    "        # Using the functional API style to create placeholders in TF2\n",
    "        S = tf.keras.Input(shape=(state_dim,), dtype=tf.float32, name='s')\n",
    "        R = tf.keras.Input(shape=(1,), dtype=tf.float32, name='r')\n",
    "        S_ = tf.keras.Input(shape=(state_dim,), dtype=tf.float32, name='s_')\n",
    "\n",
    "        # Assigning to DDPG_CLASS\n",
    "        DDPG_CLASS.S = S\n",
    "        DDPG_CLASS.R = R\n",
    "        DDPG_CLASS.S_ = S_\n",
    "\n",
    "        # Actor and Critic initialization\n",
    "        actor = DDPG_CLASS.Actor(state_dim, action_dim, action_bound, LR_A, REPLACEMENT)\n",
    "        critic = DDPG_CLASS.Critic(state_dim, action_dim, LR_C, GAMMA, REPLACEMENT, actor.a, actor.a_)\n",
    "        actor.add_grad_to_graph(critic.a_grads)\n",
    "        M = DDPG_CLASS.Memory(MEMORY_CAPACITY, dims=2 * state_dim + action_dim + 1)\n",
    "\n",
    "        # Initializing variables\n",
    "        checkpoint = tf.train.Checkpoint(actor=actor, critic=critic)\n",
    "        checkpoint_manager = tf.train.CheckpointManager(checkpoint, \"folder_for_nn_noise/EH_save_net\", max_to_keep=100)\n",
    "\n",
    "        if OUTPUT_GRAPH:\n",
    "            tf.summary.create_file_writer(\"logs/\")\n",
    "\n",
    "        print(\"modulation=\", modulation, \"snr=\", snr)\n",
    "\n",
    "        for i in range(MAX_EPISODES):\n",
    "    \n",
    "            s = env.reset_P2P(snr)\n",
    "\n",
    "            ep_reward = 0\n",
    "            for j in range(MAX_EP_STEPS):\n",
    "                a = actor.choose_action(s)\n",
    "                a = np.random.normal(a, var)\n",
    "                a = np.clip(a, 0, 1)\n",
    "                s_, r, info = env.step_P2P([a, modulation]) # input modulation 0:qpsk,1:8psk,2:16qam\n",
    "\n",
    "                M.store_transition(s, a, r , s_)\n",
    "\n",
    "                if M.pointer > MEMORY_CAPACITY:\n",
    "                    # tip and tip2 are only for printing\n",
    "                    if tip == 1:\n",
    "                        print(\"memory full\", j, i)\n",
    "                        tip = 0\n",
    "                    var *= 0.9995  # decay the action randomness\n",
    "                    if tip2 == 1 and var < 0.00000001:\n",
    "                        print(\"var zero\", j, i)\n",
    "                        tip2 = 0\n",
    "                    \n",
    "                    b_M = M.sample(BATCH_SIZE)\n",
    "                    b_s = b_M[:, :state_dim]\n",
    "                    b_a = b_M[:, state_dim: state_dim + action_dim]\n",
    "                    b_r = b_M[:, -state_dim - 1: -state_dim]\n",
    "                    b_s_ = b_M[:, -state_dim:]\n",
    "\n",
    "                    critic.learn(b_s, b_a, b_r, b_s_)\n",
    "                    actor.learn(b_s)\n",
    "\n",
    "                s = s_\n",
    "                \n",
    "                ep_reward += r\n",
    "\n",
    "            if i % 30 == 0:\n",
    "                print(\"net bit rate=\", r, \"action\", a, \"solar,channel,battery\", s, \"epoch\", i)\n",
    "                print(\"ave_reward\", ep_reward / (j + 1))\n",
    "                \n",
    "        # Saving checkpoint\n",
    "        save_path = checkpoint_manager.save()\n",
    "        print(\"Save to path: \", save_path)\n",
    "\n",
    "print(\"----------------------------END--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
